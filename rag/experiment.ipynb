{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from rich import print\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from unstructured.cleaners.core import clean_extra_whitespace, group_broken_paragraphs\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    UnstructuredFileLoader(\n",
    "        \"/teamspace/studios/this_studio/data/2401.08406.pdf\",\n",
    "        post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "    ),\n",
    "    UnstructuredFileLoader(\n",
    "        \"/teamspace/studios/this_studio/data/2401.00908.pdf\",\n",
    "        post_processors=[clean_extra_whitespace, group_broken_paragraphs],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(\n",
    "        loader.load_and_split(text_splitter=text_splitter),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\nThere are two common ways in which developers are incorporating proprietary and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Genera- tion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">seen much penetration of AI, and we study a potentially disruptive application - what if we could provide </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">other industrial domains.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'\\n\\nThere are two common ways in which developers are incorporating proprietary and \u001b[0m\n",
       "\u001b[32mdomain-specific data when building applications of Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: Retrieval-Augmented Genera- tion \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the \u001b[0m\n",
       "\u001b[32madditional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. \u001b[0m\n",
       "\u001b[32mIn this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple \u001b[0m\n",
       "\u001b[32mpopular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including \u001b[0m\n",
       "\u001b[32mextracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging \u001b[0m\n",
       "\u001b[32mGPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and \u001b[0m\n",
       "\u001b[32mfine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not \u001b[0m\n",
       "\u001b[32mseen much penetration of AI, and we study a potentially disruptive application - what if we could provide \u001b[0m\n",
       "\u001b[32mlocation-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in \u001b[0m\n",
       "\u001b[32mcapturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We \u001b[0m\n",
       "\u001b[32msee an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases\u001b[0m\n",
       "\u001b[32maccuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages \u001b[0m\n",
       "\u001b[32minformation from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. \u001b[0m\n",
       "\u001b[32mOverall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge \u001b[0m\n",
       "\u001b[32macross a dimension that is critical for a specific industry, paving the way for further applications of LLMs in \u001b[0m\n",
       "\u001b[32mother industrial domains.'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95eeb17d7cd4350a1de01468c82a647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ae41fd5cb14636a7a5123ab7f53627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92b8a3596744b5087c36e8c3fd1683a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/92.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23317999562046e1bfdd048dc8a5500b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e5d8328cfa45569944d4c65b45f975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b09d5837444c60a5141ee3b86646de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981190b1a04340e8ab4a4b7391968e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3e45a01ac745858bc10e27e2d60204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7f7cbe1beb489a9834b3966042df3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b77b331e3de426ba542b23aa4db61f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1251e9e74474ecb8d265dbfb73e65f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings' : \"True\"}\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name = model_name,\n",
    "    model_kwargs = model_kwargs,\n",
    "    encode_kwargs = encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents = docs, embedding = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fc8b959f594d02bc4539968369a326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8152b7c6b46403b896792616c2e4752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebc35ae58a648d8bdf1d48f9eb9777c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d41c0da48bb43e092206f4d44708b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b76d67d5ed491c80ae9cb42eb218ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a64334e9f14ea1a9e685d6957b2e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker_model = CrossEncoder(model_name=\"BAAI/bge-reranker-large\", max_length=512)\n",
    "\n",
    "\n",
    "def rerank_docs(query, retrieved_docs):\n",
    "    query_and_docs = [(query, r.page_content) for r in retrieved_docs]\n",
    "    scores = reranker_model.predict(query_and_docs)\n",
    "    return sorted(list(zip(retrieved_docs, scores)), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(results, *scores):\n",
    "    if scores:\n",
    "        results = zip(results, scores)\n",
    "    for result in results:\n",
    "        if isinstance(result,tuple):\n",
    "            print(result[1])\n",
    "            print(result[0])\n",
    "        else:\n",
    "            print(result)\n",
    "        print('\\n--------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = 'Where was the agricultural dataset collected for the USA?'\n",
    "query2 = \"Where was the agricultural dataset collected for the India?\"\n",
    "query3 = 'How many pdf were used to collect dataset?'\n",
    "query4 = \"What are the metrics used to evaluate the answers?\"\n",
    "query5 = 'how was the content and structure of available document augmented?'\n",
    "query6 = 'What was the answer generation process used in the Paper?'\n",
    "query7 = \"How many pdf data were collected from the USA?\"\n",
    "query8 = 'What is the DocLLM architecture ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: Query -&gt;  Where was the agricultural dataset collected for the USA?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example \u001b[1;36m1\u001b[0m: Query ->  Where was the agricultural dataset collected for the USA?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...................................................................................................</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrived document:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrived document:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> Dataset Overview\n",
       "\n",
       "This study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers \n",
       "originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using \n",
       "agriculture as the industrial setting as an example. Available data varied considerably in format and content, \n",
       "ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we\n",
       "present each dataset in more detail.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span> USA\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m Dataset Overview\n",
       "\n",
       "This study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers \n",
       "originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using \n",
       "agriculture as the industrial setting as an example. Available data varied considerably in format and content, \n",
       "ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we\n",
       "present each dataset in more detail.\n",
       "\n",
       "\u001b[1;36m3.1\u001b[0m USA\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: Query -&gt;  Where was the agricultural dataset collected for the India?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example \u001b[1;36m2\u001b[0m: Query ->  Where was the agricultural dataset collected for the India?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...................................................................................................</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrived document:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrived document:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> Dataset Overview\n",
       "\n",
       "This study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers \n",
       "originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using \n",
       "agriculture as the industrial setting as an example. Available data varied considerably in format and content, \n",
       "ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we\n",
       "present each dataset in more detail.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span> USA\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3\u001b[0m Dataset Overview\n",
       "\n",
       "This study evaluates fine-tuned and RAG-enhanced language models using context-related questions and answers \n",
       "originated datasets from three major crop producer countries: USA, Brazil and India. In our case, we are using \n",
       "agriculture as the industrial setting as an example. Available data varied considerably in format and content, \n",
       "ranging from regulatory documents, scientific reports, agronomic exams, to knowledge databases. In this section, we\n",
       "present each dataset in more detail.\n",
       "\n",
       "\u001b[1;36m3.1\u001b[0m USA\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>: Query -&gt;  How many pdf were used to collect dataset?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example \u001b[1;36m3\u001b[0m: Query ->  How many pdf were used to collect dataset?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...................................................................................................</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrived document:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrived document:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span> USA\n",
       "\n",
       "We collected documents, handbooks, and reports publicly available online from the United States Department of \n",
       "Agriculture <span style=\"font-weight: bold\">(</span>USDA<span style=\"font-weight: bold\">)</span>, state agriculture and consumer services agencies, and partners from the Land-Grant Institutions\n",
       "National Program. Available documents contain federal regulatory and policy information surrounding crop and \n",
       "livestock management, information on diseases and best practices, quality assurance and export regulations, details\n",
       "on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files \n",
       "with over 50M tokens, representing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span> states in the USA. We downloaded and preprocessed these files, extracting the\n",
       "textual information that could be used as input to the Q&amp;A generation pipeline. To benchmark and evaluate the \n",
       "models, we employed the documents related to the Washington state, which comprises <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">573</span> files with over 2M tokens. \n",
       "We present in Listing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> an example of content within these documents.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3.1\u001b[0m USA\n",
       "\n",
       "We collected documents, handbooks, and reports publicly available online from the United States Department of \n",
       "Agriculture \u001b[1m(\u001b[0mUSDA\u001b[1m)\u001b[0m, state agriculture and consumer services agencies, and partners from the Land-Grant Institutions\n",
       "National Program. Available documents contain federal regulatory and policy information surrounding crop and \n",
       "livestock management, information on diseases and best practices, quality assurance and export regulations, details\n",
       "on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files \n",
       "with over 50M tokens, representing \u001b[1;36m44\u001b[0m states in the USA. We downloaded and preprocessed these files, extracting the\n",
       "textual information that could be used as input to the Q&A generation pipeline. To benchmark and evaluate the \n",
       "models, we employed the documents related to the Washington state, which comprises \u001b[1;36m573\u001b[0m files with over 2M tokens. \n",
       "We present in Listing \u001b[1;36m5\u001b[0m an example of content within these documents.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>: Query -&gt;  What are the metrics used to evaluate the answers?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example \u001b[1;36m4\u001b[0m: Query ->  What are the metrics used to evaluate the answers?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...................................................................................................</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrived document:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrived document:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Details: We assess the level of detail in both the generated questions and answers by counting the number of tokens\n",
       "<span style=\"font-weight: bold\">(</span>words<span style=\"font-weight: bold\">)</span> in each. This metric provides insight into the depth and specificity of the content generated by the Q&amp;A \n",
       "system. By employing these metrics, we can effectively evaluate and refine the Q&amp;A generation process, ensuring \n",
       "that the generated content is informative, relevant, diverse, and grounded in the source material. This will \n",
       "ultimately lead to a more useful and effective Q&amp;A generation system for the target audience.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Details: We assess the level of detail in both the generated questions and answers by counting the number of tokens\n",
       "\u001b[1m(\u001b[0mwords\u001b[1m)\u001b[0m in each. This metric provides insight into the depth and specificity of the content generated by the Q&A \n",
       "system. By employing these metrics, we can effectively evaluate and refine the Q&A generation process, ensuring \n",
       "that the generated content is informative, relevant, diverse, and grounded in the source material. This will \n",
       "ultimately lead to a more useful and effective Q&A generation system for the target audience.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>: Query -&gt;  how was the content and structure of available document augmented?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example \u001b[1;36m5\u001b[0m: Query ->  how was the content and structure of available document augmented?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...................................................................................................</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrived document:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrived document:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">First, we augment the content and structure of available documents by explicitly adding supporting tags from the \n",
       "text. We formulated prompts to extract a list of locations and agronomic topics mentioned in each section of the \n",
       "document <span style=\"font-weight: bold\">(</span>e.g., if that section refers to crops, cattle, or diseases<span style=\"font-weight: bold\">)</span>, as exemplified in Listing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, and task the \n",
       "LLM model to answering them based on the data extracted from the JSON files. The aim is to use of the the \n",
       "additional information, including locations and mentioned topics, to ground the generation process, enhancing the \n",
       "relevance of the questions and guiding the model to cover a wide range of topics and challenges.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"grobid_version\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"0.7.3\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"grobid_timestamp\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"2023-07-04T13:05+0000\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"language_code\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"en\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"citations\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"authors\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"given_name\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"J\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"surname\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Abatzoglou\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"name\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"J T Abatzoglou\"</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span>\n",
       "\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"given_name\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"T\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"surname\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Brown\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"name\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"T J Brown\"</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "First, we augment the content and structure of available documents by explicitly adding supporting tags from the \n",
       "text. We formulated prompts to extract a list of locations and agronomic topics mentioned in each section of the \n",
       "document \u001b[1m(\u001b[0me.g., if that section refers to crops, cattle, or diseases\u001b[1m)\u001b[0m, as exemplified in Listing \u001b[1;36m3\u001b[0m, and task the \n",
       "LLM model to answering them based on the data extracted from the JSON files. The aim is to use of the the \n",
       "additional information, including locations and mentioned topics, to ground the generation process, enhancing the \n",
       "relevance of the questions and guiding the model to cover a wide range of topics and challenges.\n",
       "\n",
       "\u001b[1;36m7\u001b[0m\n",
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "\n",
       "\u001b[32m\"grobid_version\"\u001b[0m: \u001b[32m\"0.7.3\"\u001b[0m, \u001b[32m\"grobid_timestamp\"\u001b[0m: \u001b[32m\"2023-07-04T13:05+0000\"\u001b[0m, \u001b[32m\"language_code\"\u001b[0m: \u001b[32m\"en\"\u001b[0m, \u001b[32m\"citations\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "\n",
       "\u001b[32m\"authors\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "\n",
       "\u001b[32m\"given_name\"\u001b[0m: \u001b[32m\"J\"\u001b[0m, \u001b[32m\"surname\"\u001b[0m: \u001b[32m\"Abatzoglou\"\u001b[0m, \u001b[32m\"name\"\u001b[0m: \u001b[32m\"J T Abatzoglou\"\u001b[0m\n",
       "\n",
       "\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\n",
       "\n",
       "\u001b[32m\"given_name\"\u001b[0m: \u001b[32m\"T\"\u001b[0m, \u001b[32m\"surname\"\u001b[0m: \u001b[32m\"Brown\"\u001b[0m, \u001b[32m\"name\"\u001b[0m: \u001b[32m\"T J Brown\"\u001b[0m\n",
       "\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>: Query -&gt;  What was the answer generation process used in the Paper?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example \u001b[1;36m6\u001b[0m: Query ->  What was the answer generation process used in the Paper?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...................................................................................................</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrived document:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrived document:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer generation: with the question and the retrieved chunks as input, an LLM model was used to synthesize an \n",
       "answer. Specifically, we provided the retrieved information from the FAISS database to GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> as context within a \n",
       "custom prompt, which allowed the generation of domain-specific answers. The answers were properly formatted in a \n",
       "JSON file along with the associated questions to create the Q&amp;A pair.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">{{</span>#system~<span style=\"font-weight: bold\">}}</span> You are an expert in agriculture and you are formulating questions from documents to assess\n",
       "\n",
       "the knowledge of a student about agriculture-related topics.\n",
       "\n",
       "You have access to the following document metadata encoded as JSON: <span style=\"font-weight: bold\">{{</span>context<span style=\"font-weight: bold\">}}</span> <span style=\"font-weight: bold\">{{</span>~<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">system</span><span style=\"font-weight: bold\">}}</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">{{</span>#user~<span style=\"font-weight: bold\">}}</span> An example of expected output follows:\n",
       "\n",
       "Examples: <span style=\"font-weight: bold\">{</span>examples<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "The document is from <span style=\"font-weight: bold\">{{</span>source<span style=\"font-weight: bold\">}}</span> and has title: <span style=\"font-weight: bold\">{{</span>title<span style=\"font-weight: bold\">}}</span>\n",
       "\n",
       "Add location <span style=\"font-weight: bold\">(</span>state, country<span style=\"font-weight: bold\">)</span> and crop information to each question, if possible.\n",
       "\n",
       "Please formulate as many questions as possible to assess knowledge of the text below.\n",
       "\n",
       "<span style=\"font-weight: bold\">{{</span>section<span style=\"font-weight: bold\">}}</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">{{</span>~<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">user</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Answer generation: with the question and the retrieved chunks as input, an LLM model was used to synthesize an \n",
       "answer. Specifically, we provided the retrieved information from the FAISS database to GPT-\u001b[1;36m4\u001b[0m as context within a \n",
       "custom prompt, which allowed the generation of domain-specific answers. The answers were properly formatted in a \n",
       "JSON file along with the associated questions to create the Q&A pair.\n",
       "\n",
       "\u001b[1;36m9\u001b[0m\n",
       "\n",
       "\u001b[1m{\u001b[0m\u001b[1m{\u001b[0m#system~\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m You are an expert in agriculture and you are formulating questions from documents to assess\n",
       "\n",
       "the knowledge of a student about agriculture-related topics.\n",
       "\n",
       "You have access to the following document metadata encoded as JSON: \u001b[1m{\u001b[0m\u001b[1m{\u001b[0mcontext\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m \u001b[1m{\u001b[0m\u001b[1m{\u001b[0m~\u001b[35m/\u001b[0m\u001b[95msystem\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n",
       "\n",
       "\u001b[1m{\u001b[0m\u001b[1m{\u001b[0m#user~\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m An example of expected output follows:\n",
       "\n",
       "Examples: \u001b[1m{\u001b[0mexamples\u001b[1m}\u001b[0m\n",
       "\n",
       "The document is from \u001b[1m{\u001b[0m\u001b[1m{\u001b[0msource\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m and has title: \u001b[1m{\u001b[0m\u001b[1m{\u001b[0mtitle\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n",
       "\n",
       "Add location \u001b[1m(\u001b[0mstate, country\u001b[1m)\u001b[0m and crop information to each question, if possible.\n",
       "\n",
       "Please formulate as many questions as possible to assess knowledge of the text below.\n",
       "\n",
       "\u001b[1m{\u001b[0m\u001b[1m{\u001b[0msection\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n",
       "\n",
       "\u001b[1m{\u001b[0m\u001b[1m{\u001b[0m~\u001b[35m/\u001b[0m\u001b[95muser\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>: Query -&gt;  How many pdf data were collected from the USA?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example \u001b[1;36m7\u001b[0m: Query ->  How many pdf data were collected from the USA?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...................................................................................................</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrived document:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrived document:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span> USA\n",
       "\n",
       "We collected documents, handbooks, and reports publicly available online from the United States Department of \n",
       "Agriculture <span style=\"font-weight: bold\">(</span>USDA<span style=\"font-weight: bold\">)</span>, state agriculture and consumer services agencies, and partners from the Land-Grant Institutions\n",
       "National Program. Available documents contain federal regulatory and policy information surrounding crop and \n",
       "livestock management, information on diseases and best practices, quality assurance and export regulations, details\n",
       "on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files \n",
       "with over 50M tokens, representing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span> states in the USA. We downloaded and preprocessed these files, extracting the\n",
       "textual information that could be used as input to the Q&amp;A generation pipeline. To benchmark and evaluate the \n",
       "models, we employed the documents related to the Washington state, which comprises <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">573</span> files with over 2M tokens. \n",
       "We present in Listing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> an example of content within these documents.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m3.1\u001b[0m USA\n",
       "\n",
       "We collected documents, handbooks, and reports publicly available online from the United States Department of \n",
       "Agriculture \u001b[1m(\u001b[0mUSDA\u001b[1m)\u001b[0m, state agriculture and consumer services agencies, and partners from the Land-Grant Institutions\n",
       "National Program. Available documents contain federal regulatory and policy information surrounding crop and \n",
       "livestock management, information on diseases and best practices, quality assurance and export regulations, details\n",
       "on assistance programs, as well as insurance and pricing guidelines. Collected data totals more than 23k PDF files \n",
       "with over 50M tokens, representing \u001b[1;36m44\u001b[0m states in the USA. We downloaded and preprocessed these files, extracting the\n",
       "textual information that could be used as input to the Q&A generation pipeline. To benchmark and evaluate the \n",
       "models, we employed the documents related to the Washington state, which comprises \u001b[1;36m573\u001b[0m files with over 2M tokens. \n",
       "We present in Listing \u001b[1;36m5\u001b[0m an example of content within these documents.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Example <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>: Query -&gt;  What is the DocLLM architecture ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Example \u001b[1;36m8\u001b[0m: Query ->  What is the DocLLM architecture ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">...................................................................................................</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrived document:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Retrived document:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "DocLLM is constructed upon the foundation of an auto-regressive transformer language model <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">]</span> following a causal \n",
       "decoder structure. It is composed of stacked transformer blocks, where each block contains a multi-head \n",
       "self-attention layer and a fully connected feed forward network. Standard language models are typically unimodal, \n",
       "accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates \n",
       "lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained using OCR.\n",
       "Simply augmenting the text with bounding box information via additive positional encoding may not capture the \n",
       "intricate relationships between text semantics and spatial layout, especially for visually rich documents <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span>. \n",
       "Consequently, we treat the spatial information about the text tokens as a distinct modality. In particular, we use \n",
       "separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer \n",
       "architecture to compute their inter-dependencies in a disentangled manner, as explained in the following section. \n",
       "Furthermore, instead of the traditional left-to-right next token prediction during self-supervised training, we \n",
       "employ a text infilling objective that better leverages contextual information.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "DocLLM is constructed upon the foundation of an auto-regressive transformer language model \u001b[1m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m following a causal \n",
       "decoder structure. It is composed of stacked transformer blocks, where each block contains a multi-head \n",
       "self-attention layer and a fully connected feed forward network. Standard language models are typically unimodal, \n",
       "accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates \n",
       "lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained using OCR.\n",
       "Simply augmenting the text with bounding box information via additive positional encoding may not capture the \n",
       "intricate relationships between text semantics and spatial layout, especially for visually rich documents \u001b[1m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m. \n",
       "Consequently, we treat the spatial information about the text tokens as a distinct modality. In particular, we use \n",
       "separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer \n",
       "architecture to compute their inter-dependencies in a disentangled manner, as explained in the following section. \n",
       "Furthermore, instead of the traditional left-to-right next token prediction during self-supervised training, we \n",
       "employ a text infilling objective that better leverages contextual information.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">metadata: \n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.00908.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "metadata: \n",
       "\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.00908.pdf'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">----------------------------------------------------------------------------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "----------------------------------------------------------------------------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries = [\n",
    "    query1, \n",
    "    query2,\n",
    "    query3,\n",
    "    query4,\n",
    "    query5,\n",
    "    query6,\n",
    "    query7,\n",
    "    query8\n",
    "\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"Example {i + 1}: Query -> \", query)\n",
    "    print('..' * 50)\n",
    "    print('Retrived document:')\n",
    "    \n",
    "    retrieved_documents = retriever.get_relevant_documents(query)\n",
    "    reranked_documents = rerank_docs(query, retrieved_documents)\n",
    "\n",
    "    print(\"--\" * 50)\n",
    "    print(reranked_documents[0][0].page_content)\n",
    "    print(\"--\" * 50)\n",
    "    print('metadata: ', reranked_documents[0][0].metadata)\n",
    "    print('--' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the DocLLM architecture ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is the DocLLM architecture ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Where was the agricultural dataset collected for the USA?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Where was the agricultural dataset collected for the USA?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6788545434853595</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0.6788545434853595\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'3.1 USA\\n\\nWe collected documents, handbooks, and reports publicly available online from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">United States Department of Agriculture (USDA), state agriculture and consumer services agencies, and partners from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">surrounding crop and livestock management, information on diseases and best practices, quality assurance and export</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">regulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these files, extracting the textual information that could be used as input to the Q&amp;A generation pipeline. To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">benchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">files with over 2M tokens. We present in Listing 5 an example of content within these documents.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'3.1 USA\\n\\nWe collected documents, handbooks, and reports publicly available online from the \u001b[0m\n",
       "\u001b[32mUnited States Department of Agriculture \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUSDA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, state agriculture and consumer services agencies, and partners from\u001b[0m\n",
       "\u001b[32mthe Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information\u001b[0m\n",
       "\u001b[32msurrounding crop and livestock management, information on diseases and best practices, quality assurance and export\u001b[0m\n",
       "\u001b[32mregulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals \u001b[0m\n",
       "\u001b[32mmore than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed \u001b[0m\n",
       "\u001b[32mthese files, extracting the textual information that could be used as input to the Q&A generation pipeline. To \u001b[0m\n",
       "\u001b[32mbenchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 \u001b[0m\n",
       "\u001b[32mfiles with over 2M tokens. We present in Listing 5 an example of content within these documents.'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(query8)\n",
    "results = db.similarity_search_with_relevance_scores(query1, k = 1)\n",
    "print(query1)\n",
    "pretty_print_docs(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Where was the agricultural dataset collected for the India?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Where was the agricultural dataset collected for the India?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6788545434853595</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0.6788545434853595\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'3.1 USA\\n\\nWe collected documents, handbooks, and reports publicly available online from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">United States Department of Agriculture (USDA), state agriculture and consumer services agencies, and partners from</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">surrounding crop and livestock management, information on diseases and best practices, quality assurance and export</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">regulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">more than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these files, extracting the textual information that could be used as input to the Q&amp;A generation pipeline. To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">benchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">files with over 2M tokens. We present in Listing 5 an example of content within these documents.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.08406.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'3.1 USA\\n\\nWe collected documents, handbooks, and reports publicly available online from the \u001b[0m\n",
       "\u001b[32mUnited States Department of Agriculture \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUSDA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, state agriculture and consumer services agencies, and partners from\u001b[0m\n",
       "\u001b[32mthe Land-Grant Institutions National Program. Available documents contain federal regulatory and policy information\u001b[0m\n",
       "\u001b[32msurrounding crop and livestock management, information on diseases and best practices, quality assurance and export\u001b[0m\n",
       "\u001b[32mregulations, details on assistance programs, as well as insurance and pricing guidelines. Collected data totals \u001b[0m\n",
       "\u001b[32mmore than 23k PDF files with over 50M tokens, representing 44 states in the USA. We downloaded and preprocessed \u001b[0m\n",
       "\u001b[32mthese files, extracting the textual information that could be used as input to the Q&A generation pipeline. To \u001b[0m\n",
       "\u001b[32mbenchmark and evaluate the models, we employed the documents related to the Washington state, which comprises 573 \u001b[0m\n",
       "\u001b[32mfiles with over 2M tokens. We present in Listing 5 an example of content within these documents.'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.08406.pdf'\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "--------\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "--------\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "results = db.similarity_search_with_relevance_scores(query1, k = 1)\n",
    "print(query2)\n",
    "pretty_print_docs(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the DocLLM architecture ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is the DocLLM architecture ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'We adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning it on instruction data curated from several datasets. These tasks encompass key information extraction,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural language inference, visual question-answering and document classification. Our instruction-tuning data </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">integrated during instruction-tuning to facilitate learning the logical structure of the documents. We observe that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the modifications introduced by DocLLM result in a performance improvement ranging from 15% to 61% for the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama2-7B model in four out of five previously unseen datasets.\\n\\nFig. 1 summarizes the framework. Our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">contributions include:\\n\\n1. A light-weight extension to LLMs designed for understanding visual documents.\\n\\n2. A </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.00908.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'We adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by \u001b[0m\n",
       "\u001b[32mfine-tuning it on instruction data curated from several datasets. These tasks encompass key information extraction,\u001b[0m\n",
       "\u001b[32mnatural language inference, visual question-answering and document classification. Our instruction-tuning data \u001b[0m\n",
       "\u001b[32mcovers both single and multi-page documents. Layout hints such as field separators, titles and captions can be \u001b[0m\n",
       "\u001b[32mintegrated during instruction-tuning to facilitate learning the logical structure of the documents. We observe that\u001b[0m\n",
       "\u001b[32mthe modifications introduced by DocLLM result in a performance improvement ranging from 15% to 61% for the \u001b[0m\n",
       "\u001b[32mLlama2-7B model in four out of five previously unseen datasets.\\n\\nFig. 1 summarizes the framework. Our \u001b[0m\n",
       "\u001b[32mcontributions include:\\n\\n1. A light-weight extension to LLMs designed for understanding visual documents.\\n\\n2. A \u001b[0m\n",
       "\u001b[32mdisentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.'\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.00908.pdf'\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(query8)\n",
    "results = db.similarity_search_with_relevance_scores(query8, k = 1)\n",
    "for r in results:\n",
    "    print(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n\\nDocLLM is constructed upon the foundation of an auto-regressive transformer language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model [4] following a causal decoder structure. It is composed of stacked transformer blocks, where each block </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">contains a multi-head self-attention layer and a fully connected feed forward network. Standard language models are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">typically unimodal, accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">obtained using OCR. Simply augmenting the text with bounding box information via additive positional encoding may </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">not capture the intricate relationships between text semantics and spatial layout, especially for visually rich </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">documents [10]. Consequently, we treat the spatial information about the text tokens as a distinct modality. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">particular, we use separate vectors to represent these two modalities and extend the self-attention mechanism of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the transformer architecture to compute their inter-dependencies in a disentangled manner, as explained in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">following section. Furthermore, instead of the traditional left-to-right next token prediction during </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">self-supervised training, we employ a text infilling objective that better leverages contextual information.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/teamspace/studios/this_studio/data/2401.00908.pdf'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9895678</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\n",
       "    \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mpage_content\u001b[0m=\u001b[32m'\\n\\nDocLLM is constructed upon the foundation of an auto-regressive transformer language \u001b[0m\n",
       "\u001b[32mmodel \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m following a causal decoder structure. It is composed of stacked transformer blocks, where each block \u001b[0m\n",
       "\u001b[32mcontains a multi-head self-attention layer and a fully connected feed forward network. Standard language models are\u001b[0m\n",
       "\u001b[32mtypically unimodal, accepting only a sequence of text tokens as input. In contrast, DocLLM is a multi-modal system \u001b[0m\n",
       "\u001b[32mthat integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens \u001b[0m\n",
       "\u001b[32mobtained using OCR. Simply augmenting the text with bounding box information via additive positional encoding may \u001b[0m\n",
       "\u001b[32mnot capture the intricate relationships between text semantics and spatial layout, especially for visually rich \u001b[0m\n",
       "\u001b[32mdocuments \u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Consequently, we treat the spatial information about the text tokens as a distinct modality. In \u001b[0m\n",
       "\u001b[32mparticular, we use separate vectors to represent these two modalities and extend the self-attention mechanism of \u001b[0m\n",
       "\u001b[32mthe transformer architecture to compute their inter-dependencies in a disentangled manner, as explained in the \u001b[0m\n",
       "\u001b[32mfollowing section. Furthermore, instead of the traditional left-to-right next token prediction during \u001b[0m\n",
       "\u001b[32mself-supervised training, we employ a text infilling objective that better leverages contextual information.'\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'source'\u001b[0m: \u001b[32m'/teamspace/studios/this_studio/data/2401.00908.pdf'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;36m0.9895678\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieved_documents = retriever.get_relevant_documents(query8)\n",
    "rerank_documents = rerank_docs(query8, retrieved_documents)\n",
    "print(reranked_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50757f9a1ec4f65abd3089272c3e7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce18d35de951488680f2341779c09524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb7086135954e5e875f44b8a7a248b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb894e214a60427a83ec95118579af55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa203cbbc8fe4e0e858e8d85e12f2dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329c1e7a71964dfbb6f0a72cb9d4d3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c6456644444cba888d5e1c3d29abf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2105c31114a42cea5c4f1addc220b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecff987c7d948eca9a3cd0b8dc9a3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76220e5a60f4feb9e3e28e6f1063b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64ed86738c14d11bd20c496f7e37a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from retriver import (\n",
    "    RAGException,\n",
    "    create_parent_retriever,\n",
    "    load_embedding_model,\n",
    "    load_pdf,\n",
    "    load_reranker_model,\n",
    "    retrieve_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client import RAGClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = RAGClient(\"/teamspace/studios/this_studio/data/2401.08406.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contexts': [(Document(page_content='Listing 3: Prompt used to identify supporting context (i.e., list of locations and agronomic topics mentioned) from a document during question generation.\\n\\n2.4 Answer Generation\\n\\nWe employ Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), which is an innovative approach that combines the power of retrieval and generation mechanisms, to create high-quality answers. RAG is particularly useful when dealing with large and complex datasets, as it can effectively recover relevant information associated with a query and use it to enhance the generation process.\\n\\nThe RAG pipeline begins by retrieving, for a given question, the most relevant documents or passages from our dataset. The retrieval system employs techniques such as BM25, Dense Retrieval (Reimers and Gurevych, 2019; Ni et al., 2022), and other advanced retrieval mechanisms. The retrieved documents serve as a knowledge source for the subsequent generation phase. Once the relevant passages have been identified, the generation component comes into play. An LLM takes the question and the retrieved information as inputs and generates a contextually appropriate answer. The generation process is guided by the context provided by the retrieved documents, ensuring that the generated Q&A pairs are accurate, relevant, and informative. More specifically, the generation process follows three steps:\\n\\nEmbedding generation and index construction: we compute embeddings from text chunks extracted from the PDF documents in our dataset, using sentence transformers (Reimers and Gurevych, 2019). We then used Facebook AI Similarity Search (FAISS) (Johnson et al., 2019), a library for efficient indexing and similarity search of vectors, to create a database of the embeddings.', metadata={'source': '/teamspace/studios/this_studio/data/2401.08406.pdf'}),\n",
       "   0.9253659)],\n",
       " 'response': \"Based on the context provided, RAG stands for Retrieval-Augmented Generation, which is an innovative approach used in answer generation. It combines retrieval and generation mechanisms to create high-quality answers, especially when dealing with large and complex datasets. The pipeline begins by retrieving relevant documents or passages for a given question using techniques like BM25 and Dense Retrieval. The retrieved documents serve as a knowledge source for the subsequent generation phase, where an LLM takes the question and the retrieved information as inputs to generate contextually appropriate answers guided by the retrieved documents' context. Embeddings are generated from text chunks extracted from the PDF documents using sentence transformers, and Facebook AI Similarity Search is used to create a database for efficient indexing and similarity search of vectors.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.generate('what is rag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-19 09:47:44.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclient\u001b[0m:\u001b[36mstream\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mListing 3: Prompt used to identify supporting context (i.e., list of locations and agronomic topics mentioned) from a document during question generation.\n",
      "\n",
      "2.4 Answer Generation\n",
      "\n",
      "We employ Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), which is an innovative approach that combines the power of retrieval and generation mechanisms, to create high-quality answers. RAG is particularly useful when dealing with large and complex datasets, as it can effectively recover relevant information associated with a query and use it to enhance the generation process.\n",
      "\n",
      "The RAG pipeline begins by retrieving, for a given question, the most relevant documents or passages from our dataset. The retrieval system employs techniques such as BM25, Dense Retrieval (Reimers and Gurevych, 2019; Ni et al., 2022), and other advanced retrieval mechanisms. The retrieved documents serve as a knowledge source for the subsequent generation phase. Once the relevant passages have been identified, the generation component comes into play. An LLM takes the question and the retrieved information as inputs and generates a contextually appropriate answer. The generation process is guided by the context provided by the retrieved documents, ensuring that the generated Q&A pairs are accurate, relevant, and informative. More specifically, the generation process follows three steps:\n",
      "\n",
      "Embedding generation and index construction: we compute embeddings from text chunks extracted from the PDF documents in our dataset, using sentence transformers (Reimers and Gurevych, 2019). We then used Facebook AI Similarity Search (FAISS) (Johnson et al., 2019), a library for efficient indexing and similarity search of vectors, to create a database of the embeddings.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, RAG (Retrieval-Augmented Generation) is an approach that combines retrieval and generation mechanisms to create high-quality answers for given questions. It begins with retrieving relevant documents or passages using techniques like BM25, Dense Retrieval, and advanced retrieval mechanisms. The retrieved information serves as a knowledge source for the subsequent generation phase by guiding the LLM (Language Learning Model) that takes the question and the retrieved documents as inputs to generate contextually appropriate answers. This approach ensures accurate, relevant, and informative Q&A pairs by being guided by the context provided by the retrieved documents. The first step in this process involves computing embeddings from text chunks extracted from PDF documents using sentence transformers and creating a database of these embeddings using Facebook AI Similarity Search for efficient indexing and similarity search of vectors."
     ]
    }
   ],
   "source": [
    "for r in c.stream('what is rag'):\n",
    "    print(r , end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
